# ğŸ“˜ Machine Learning æ ¸å¿ƒç­†è¨˜æ•´ç†

**å®Œæ•´æ©Ÿå™¨å­¸ç¿’ç­†è¨˜ - å¾é›¶é–‹å§‹ç†è§£æ‰€æœ‰æ¦‚å¿µ**

æœ¬ç­†è¨˜æä¾›å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’æ ¸å¿ƒæ¦‚å¿µï¼Œé…åˆ Python è¦–è¦ºåŒ–è¼”åŠ©ç†è§£ã€‚æ‰€æœ‰åœ–ç‰‡å’Œå½±ç‰‡éƒ½å¯ä»¥é€éåŸ·è¡Œè…³æœ¬è‡ªå‹•ç”Ÿæˆã€‚

---

## ğŸ§  ç¬¬ä¸€éƒ¨åˆ†ï¼šå¿«é€Ÿè¨˜æ†¶ï¼ˆæ¦‚å¿µç†è§£ï¼‰

### 1ï¸âƒ£ åŸºç¤èˆ‡å›æ­¸ï¼ˆRegression & Basicsï¼‰

#### **Least Squares Error (LSE)**
- **ç‚ºä½•åˆç†**ï¼šå‡è¨­èª¤å·®æœå¾é«˜æ–¯åˆ†ä½ˆï¼ˆGaussian Distributionï¼‰
- **ç›®æ¨™**ï¼šæœ€å°åŒ–é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼ä¹‹é–“çš„å¹³æ–¹å·®ç¸½å’Œ
- **æ•¸å­¸è¡¨ç¤º**ï¼š$E_D(w) = \frac{1}{2}\sum_{n=1}^{N} (y_n - w^T\phi(x_n))^2$

**ç›´è§€ç†è§£**ï¼š
LSE å°±åƒåœ¨æ‰¾ä¸€æ¢æœ€èƒ½ä»£è¡¨æ•¸æ“šè¶¨å‹¢çš„ç·šã€‚æ¯å€‹é»åˆ°ç·šçš„è·é›¢çš„å¹³æ–¹åŠ èµ·ä¾†æœ€å°çš„é‚£æ¢ç·šå°±æ˜¯æˆ‘å€‘è¦çš„ã€‚

#### **Regularizationï¼ˆæ­£è¦åŒ–ï¼‰**
æ­£è¦åŒ–æ˜¯é˜²æ­¢æ¨¡å‹éåº¦æ“¬åˆçš„æŠ€è¡“ï¼š

- **L2 Regularization (Ridge)**
  - ä½¿æ¬Šé‡ $w$ è®Šå°ä½†ä¸ç‚ºé›¶
  - å…¬å¼ï¼š$E(w) = E_D(w) + \frac{\lambda}{2} w^T w$
  - æ•ˆæœï¼šè®“æ¨¡å‹æ›´å¹³æ»‘ï¼Œæ¸›å°‘å°è¨“ç·´æ•¸æ“šçš„éåº¦ä¾è³´

- **L1 Regularization (Lasso)**
  - ä½¿æ¬Šé‡ç¨€ç–ï¼ˆè¨±å¤šæ¬Šé‡è®Šæˆ 0ï¼‰
  - å…¬å¼ï¼š$E(w) = E_D(w) + \frac{\lambda}{2}|w|$
  - æ•ˆæœï¼šè‡ªå‹•é€²è¡Œç‰¹å¾µé¸æ“‡

**ç‚ºä»€éº¼éœ€è¦æ­£è¦åŒ–ï¼Ÿ**
æƒ³åƒä½ åœ¨è¨˜æ†¶ä¸€çµ„æ•¸å­—ã€‚å¦‚æœä½ è¨˜ä½æ¯å€‹æ•¸å­—çš„æ¯å€‹ç´°ç¯€ï¼ˆéåº¦æ“¬åˆï¼‰ï¼Œé‡åˆ°æ–°çš„ç›¸ä¼¼æ•¸å­—æ™‚åè€Œæœƒææ··ã€‚æ­£è¦åŒ–å°±æ˜¯æ•™ä½ æŠ“ä½é‡é»è€Œä¸æ˜¯æ­»è¨˜ç¡¬èƒŒã€‚

#### **Gradient Descent (GD)**
- **æ¦‚å¿µ**ï¼šæ²¿è‘—æ¢¯åº¦æœ€é™¡å³­çš„ç›¸åæ–¹å‘æ›´æ–°åƒæ•¸
- **æ›´æ–°è¦å‰‡**ï¼š$w^{(t+1)} = w^{(t)} - \eta \nabla E(w^{(t)})$
- **å­¸ç¿’ç‡** $\eta$ï¼šæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•·

**ç™»å±±æ¯”å–»**ï¼š
æ¢¯åº¦ä¸‹é™å°±åƒåœ¨æ¿ƒéœ§ä¸­ä¸‹å±±ã€‚ä½ çœ‹ä¸åˆ°å±±åº•åœ¨å“ªï¼Œä½†å¯ä»¥æ„Ÿè¦ºåˆ°å“ªå€‹æ–¹å‘æœ€é™¡ã€‚æ¯æ¬¡éƒ½å¾€æœ€é™¡çš„æ–¹å‘èµ°ä¸€å°æ­¥ï¼Œæœ€çµ‚æœƒåˆ°é”å±±åº•ï¼ˆæœ€å°å€¼ï¼‰ã€‚

#### **Newton's Method**
- **ç‰¹é»**ï¼šä½¿ç”¨äºŒéšå°æ•¸ï¼ˆHessian çŸ©é™£ï¼‰åŠ é€Ÿæ”¶æ–‚
- **æ›´æ–°è¦å‰‡**ï¼š$w^{(t+1)} = w^{(t)} - H^{-1}\nabla E(w^{(t)})$
- **å„ªé»**ï¼šæ”¶æ–‚é€Ÿåº¦å¿«
- **ç¼ºé»**ï¼šè¨ˆç®— Hessian çŸ©é™£å’Œå…¶é€†çŸ©é™£æˆæœ¬é«˜
- **æ³¨æ„**ï¼šè‹¥ Hessian å¥‡ç•°ï¼ˆsingularï¼‰ï¼Œæ‡‰æ”¹ç”¨æ¢¯åº¦ä¸‹é™

#### **Biasâ€“Variance Trade-off**
é€™æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­æœ€é‡è¦çš„æ¦‚å¿µä¹‹ä¸€ï¼š

- **é«˜åå·®ï¼ˆHigh Biasï¼‰**
  - æ¨¡å‹å¤ªç°¡å–®
  - ç„¡æ³•æ•æ‰æ•¸æ“šçš„çœŸå¯¦æ¨¡å¼
  - çµæœï¼šæ¬ æ“¬åˆï¼ˆUnderfittingï¼‰
  - ä¾‹å­ï¼šç”¨ç›´ç·šæ“¬åˆæ›²ç·šæ•¸æ“š

- **é«˜è®Šç•°ï¼ˆHigh Varianceï¼‰**
  - æ¨¡å‹å¤ªè¤‡é›œ
  - å°è¨“ç·´æ•¸æ“šéåº¦æ•æ„Ÿ
  - çµæœï¼šéåº¦æ“¬åˆï¼ˆOverfittingï¼‰
  - ä¾‹å­ï¼šç”¨é«˜æ¬¡å¤šé …å¼æ“¬åˆå°‘é‡æ•¸æ“šé»

**å°„ç®­æ¯”å–»**ï¼š
- é«˜åå·®ï¼šç®­éƒ½åé›¢é¶å¿ƒï¼Œä½†å¾ˆé›†ä¸­ â†’ ç„æº–æœ‰å•é¡Œ
- é«˜è®Šç•°ï¼šç®­åˆ†æ•£åœ¨é¶å­å„è™• â†’ ä¸ç©©å®š
- ç†æƒ³ç‹€æ…‹ï¼šç®­éƒ½é›†ä¸­åœ¨é¶å¿ƒé™„è¿‘

---

### 2ï¸âƒ£ æ©Ÿç‡èˆ‡åˆ†ä½ˆï¼ˆProbability & Distributionsï¼‰

#### **Bayes' Theoremï¼ˆè²è‘‰æ–¯å®šç†ï¼‰**
$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$$

æˆ–ç°¡å¯«ç‚ºï¼š
$$Posterior \propto Likelihood \times Prior$$

**ç›´è§€ç†è§£**ï¼š
- **Priorï¼ˆå…ˆé©—ï¼‰**ï¼šåœ¨çœ‹åˆ°æ•¸æ“šä¹‹å‰ï¼Œæˆ‘å€‘å°åƒæ•¸çš„ä¿¡å¿µ
- **Likelihoodï¼ˆä¼¼ç„¶ï¼‰**ï¼šçµ¦å®šåƒæ•¸ï¼Œè§€å¯Ÿåˆ°é€™äº›æ•¸æ“šçš„å¯èƒ½æ€§
- **Posteriorï¼ˆå¾Œé©—ï¼‰**ï¼šçœ‹åˆ°æ•¸æ“šå¾Œï¼Œæ›´æ–°çš„ä¿¡å¿µ

**ä¾‹å­**ï¼šé†«ç™‚è¨ºæ–·
- Prior: é€™å€‹ç–¾ç—…çš„ç™¼ç—…ç‡æ˜¯ 1%
- Likelihood: å¦‚æœæœ‰ç—…ï¼Œæ¸¬è©¦é™½æ€§çš„æ©Ÿç‡æ˜¯ 99%
- Posterior: æ¸¬è©¦é™½æ€§å¾Œï¼ŒçœŸçš„æœ‰ç—…çš„æ©Ÿç‡æ˜¯å¤šå°‘ï¼Ÿ

#### **MLE (Maximum Likelihood Estimation)**
- **ç›®æ¨™**ï¼šæ‰¾åˆ°åƒæ•¸ $\theta$ ä½¿è§€æ¸¬è³‡æ–™æ©Ÿç‡ $P(D|\theta)$ æœ€å¤§
- **æ­¥é©Ÿ**ï¼š
  1. å¯«å‡ºä¼¼ç„¶å‡½æ•¸ $L(\theta) = P(D|\theta)$
  2. å–å°æ•¸å¾—åˆ° log-likelihood $\log L(\theta)$
  3. å° $\theta$ æ±‚å°ä¸¦ä»¤å…¶ç‚ºé›¶
  4. è§£å‡º $\theta_{MLE}$

**ç¯„ä¾‹ï¼šé«˜æ–¯åˆ†ä½ˆçš„ MLE**
$$\mu_{MLE} = \frac{1}{N}\sum_{n=1}^{N} x_n$$

**ç¯„ä¾‹ï¼šPoisson åˆ†ä½ˆçš„ MLE**
$$\lambda_{MLE} = \frac{1}{N}\sum_{n=1}^{N} k_n$$

#### **MAP (Maximum A Posteriori)**
- **èˆ‡ MLE çš„ä¸åŒ**ï¼šå¼•å…¥å…ˆé©—åˆ†ä½ˆ $P(\theta)$
- **ç›®æ¨™**ï¼šæœ€å¤§åŒ– $P(\theta|D) \propto P(D|\theta)P(\theta)$
- **å„ªé»**ï¼šå¯ä»¥èå…¥å…ˆé©—çŸ¥è­˜ï¼Œé¿å…éåº¦æ“¬åˆ

**MLE vs MAP**ï¼š
- MLEï¼šå®Œå…¨æ ¹æ“šæ•¸æ“š
- MAPï¼šæ•¸æ“š + å…ˆé©—çŸ¥è­˜
- ç•¶å…ˆé©—æ˜¯å‡å‹»åˆ†ä½ˆæ™‚ï¼ŒMAP = MLE

#### **Conjugate Priorï¼ˆå…±è»›å…ˆé©—ï¼‰**
ç•¶å…ˆé©—å’Œä¼¼ç„¶çµåˆå¾Œï¼Œå¾Œé©—ä»èˆ‡å…ˆé©—å…·æœ‰ç›¸åŒçš„åˆ†ä½ˆå½¢å¼ã€‚

**Betaâ€“Binomial å…±è»›å°**
- **Prior**: $Beta(\mu|a,b)$
- **Likelihood**: $Bin(m|N,\mu)$ï¼ˆm æ¬¡æˆåŠŸï¼ŒN æ¬¡è©¦é©—ï¼‰
- **Posterior**: $Beta(\mu|a+m, b+l)$ å…¶ä¸­ $l = N-m$

**ç‚ºä»€éº¼æœ‰ç”¨ï¼Ÿ**
å…±è»›å…ˆé©—è®“è²è‘‰æ–¯æ¨æ–·è®Šå¾—ç°¡å–®ã€‚æ¯æ¬¡çœ‹åˆ°æ–°æ•¸æ“šï¼Œåªéœ€è¦æ›´æ–°åƒæ•¸ï¼Œä¸éœ€è¦é‡æ–°è¨ˆç®—è¤‡é›œçš„ç©åˆ†ã€‚

**Gammaâ€“Poisson å…±è»›å°**
- **Prior**: $Gam(\lambda|a,b)$
- **Likelihood**: $Poisson(k|\lambda)$
- **Posterior**: $Gam(\lambda|a+\sum k_i, b+N)$

#### **Gaussian Distributionï¼ˆé«˜æ–¯åˆ†ä½ˆ/å¸¸æ…‹åˆ†ä½ˆï¼‰**

**ä¸€ç¶­é«˜æ–¯åˆ†ä½ˆ**ï¼š
$$N(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

**å¤šç¶­é«˜æ–¯åˆ†ä½ˆ**ï¼š
$$N(x|\mu,\Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$$

**é‡è¦æ€§è³ª**ï¼š

1. **Isotropic Gaussianï¼ˆå„å‘åŒæ€§ï¼‰**
   - å…±è®Šç•°æ•¸çŸ©é™£ $\Sigma = \sigma^2 I$ï¼ˆå°è§’ä¸”å€¼ç›¸åŒï¼‰
   - å„ç¶­åº¦ç¨ç«‹ä¸”æœ‰ç›¸åŒçš„è®Šç•°æ•¸

2. **Affine Propertyï¼ˆä»¿å°„æ€§è³ªï¼‰**
   - è‹¥ $x \sim N(\mu, \Sigma)$ ä¸” $y = Ax + b$
   - å‰‡ $y \sim N(A\mu + b, A\Sigma A^T)$
   - **æ„ç¾©**ï¼šé«˜æ–¯åˆ†ä½ˆç¶“éç·šæ€§è½‰æ›å¾Œä»æ˜¯é«˜æ–¯åˆ†ä½ˆ

---

### 3ï¸âƒ£ åˆ†é¡èˆ‡é›†ç¾¤ï¼ˆClassification & Clusteringï¼‰

#### **Naive Bayes Classifierï¼ˆæ¨¸ç´ è²è‘‰æ–¯åˆ†é¡å™¨ï¼‰**
- **æ ¸å¿ƒå‡è¨­**ï¼šå„ç‰¹å¾µåœ¨çµ¦å®šé¡åˆ¥ä¸‹æ¢ä»¶ç¨ç«‹
- **åˆ†é¡è¦å‰‡**ï¼š
$$\hat{y} = \arg\max_k P(C_k)\prod_{i=1}^{D} P(x_i|C_k)$$

**ç‚ºä»€éº¼å«ã€Œæ¨¸ç´ ã€ï¼Ÿ**
å› ç‚ºå‡è¨­ç‰¹å¾µä¹‹é–“å®Œå…¨ç¨ç«‹ï¼Œé€™å€‹å‡è¨­å¾ˆã€Œæ¨¸ç´ ã€ï¼ˆé€šå¸¸ä¸æˆç«‹ï¼‰ï¼Œä½†å¯¦éš›æ•ˆæœå»å¾ˆå¥½ã€‚

**è™•ç†ä¸åŒé¡å‹ç‰¹å¾µ**ï¼š

1. **é›¢æ•£ç‰¹å¾µ**ï¼ˆåŠ æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰ï¼š
$$P(x_i=j|C_k) = \frac{\text{Count}(x_i=j \text{ in } C_k) + 1}{\text{Count}(C_k) + \text{TotalBins}}$$

2. **é€£çºŒç‰¹å¾µ**ï¼ˆå‡è¨­ç‚ºé«˜æ–¯åˆ†ä½ˆï¼‰ï¼š
$$P(x_i|C_k) = N(x_i|\mu_{ik}, \sigma_{ik}^2)$$

#### **Logistic Regressionï¼ˆé‚è¼¯å›æ­¸ï¼‰**
- **åå­—çš„èª¤å°**ï¼šé›–ç„¶å«å›æ­¸ï¼Œå¯¦éš›ä¸Šæ˜¯åˆ†é¡æ¨¡å‹
- **Sigmoid å‡½æ•¸**ï¼š$\sigma(a) = \frac{1}{1+e^{-a}}$
- **æ¨¡å‹**ï¼š$P(C_1|\phi) = \sigma(w^T\phi)$
- **æå¤±å‡½æ•¸**ï¼ˆäº¤å‰ç†µï¼‰ï¼š
$$E(w) = -\sum_{n=1}^{N}[t_n\ln y_n + (1-t_n)\ln(1-y_n)]$$

**ç‚ºä»€éº¼ç”¨ Sigmoidï¼Ÿ**
1. è¼¸å‡ºç¯„åœåœ¨ [0,1]ï¼Œå¯è§£é‡‹ç‚ºæ©Ÿç‡
2. å¯å¾®åˆ†ï¼Œæ–¹ä¾¿æ¢¯åº¦ä¸‹é™
3. æœ‰è‰¯å¥½çš„æ•¸å­¸æ€§è³ª

**è¨“ç·´æ–¹æ³•**ï¼š
- æ¢¯åº¦ä¸‹é™ï¼š$\nabla E(w) = \Phi^T(y-t)$
- Newton's Method / IRLSï¼šä½¿ç”¨ Hessian çŸ©é™£åŠ é€Ÿ

#### **EM Algorithm (Expectationâ€“Maximization)**
ç”¨æ–¼å«æœ‰æ½›åœ¨è®Šæ•¸æˆ–ç¼ºå¤±æ•¸æ“šæ™‚çš„åƒæ•¸ä¼°è¨ˆã€‚

**å…©æ­¥é©Ÿè¿­ä»£**ï¼š

1. **E-stepï¼ˆæœŸæœ›æ­¥é©Ÿï¼‰**
   - è¨ˆç®— responsibilities
   - ä¼°è¨ˆæ½›åœ¨è®Šæ•¸çš„å¾Œé©—æ©Ÿç‡
   - $\gamma(z_k) = P(Z=k|X,\theta^{old})$

2. **M-stepï¼ˆæœ€å¤§åŒ–æ­¥é©Ÿï¼‰**
   - å›ºå®š responsibilities
   - æœ€å¤§åŒ–åƒæ•¸ä½¿å°æ•¸ä¼¼ç„¶æœ€å¤§
   - $\theta^{new} = \arg\max_\theta Q(\theta, \theta^{old})$

**GMMï¼ˆé«˜æ–¯æ··åˆæ¨¡å‹ï¼‰ç¯„ä¾‹**ï¼š

E-step:
$$\gamma(z_{nk}) = \frac{\pi_k N(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^{K}\pi_j N(x_n|\mu_j,\Sigma_j)}$$

M-step:
$$\pi_k = \frac{N_k}{N}, \quad \mu_k = \frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})x_n$$
$$\Sigma_k = \frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})(x_n-\mu_k)(x_n-\mu_k)^T$$

å…¶ä¸­ $N_k = \sum_{n=1}^{N}\gamma(z_{nk})$

**ç‚ºä»€éº¼å« EMï¼Ÿ**
- E-step: æ ¹æ“šç•¶å‰åƒæ•¸ï¼Œ**æœŸæœ›**æ½›åœ¨è®Šæ•¸æ˜¯ä»€éº¼
- M-step: æ ¹æ“šæœŸæœ›çš„æ½›åœ¨è®Šæ•¸ï¼Œ**æœ€å¤§åŒ–**åƒæ•¸

#### **Confusion Matrixï¼ˆæ··æ·†çŸ©é™£ï¼‰**

|           | Predict Positive | Predict Negative |
|-----------|------------------|------------------|
| Actually Positive | TP (True Positive)  | FN (False Negative) |
| Actually Negative | FP (False Positive) | TN (True Negative)  |

**é‡è¦æŒ‡æ¨™**ï¼š

1. **Sensitivity (æ•æ„Ÿåº¦/å¬å›ç‡/TPR)**
   $$Sensitivity = \frac{TP}{TP + FN}$$
   - å¯¦éš›ç‚ºæ­£çš„æ¨£æœ¬ä¸­ï¼Œæœ‰å¤šå°‘è¢«æ­£ç¢ºé æ¸¬ç‚ºæ­£

2. **Specificity (ç‰¹ç•°åº¦/TNR)**
   $$Specificity = \frac{TN}{TN + FP}$$
   - å¯¦éš›ç‚ºè² çš„æ¨£æœ¬ä¸­ï¼Œæœ‰å¤šå°‘è¢«æ­£ç¢ºé æ¸¬ç‚ºè² 

3. **Precision (ç²¾ç¢ºç‡)**
   $$Precision = \frac{TP}{TP + FP}$$
   - é æ¸¬ç‚ºæ­£çš„æ¨£æœ¬ä¸­ï¼Œæœ‰å¤šå°‘çœŸçš„æ˜¯æ­£

4. **F1 Score**
   $$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$
   - ç²¾ç¢ºç‡å’Œå¬å›ç‡çš„èª¿å’Œå¹³å‡

**é†«ç™‚æª¢æ¸¬ä¾‹å­**ï¼š
- Sensitivity é«˜ï¼šä¸æœƒæ¼æ‰çœŸæ­£çš„ç—…äººï¼ˆå°‘ False Negativeï¼‰
- Specificity é«˜ï¼šä¸æœƒèª¤è¨ºå¥åº·çš„äººï¼ˆå°‘ False Positiveï¼‰

---

## ğŸ“„ ç¬¬äºŒéƒ¨åˆ†ï¼šA4 é›™é¢ç­†è¨˜ï¼ˆå…¬å¼èˆ‡æ¼”ç®—æ³•ï¼‰

### ğŸ§® Side Aï¼šRegression, Gaussian & Bayes

#### (1) Linear Regression

**åŸºæœ¬æ¨¡å‹**ï¼š
$$y(x, w) = w^T \phi(x)$$

**æå¤±å‡½æ•¸**ï¼š
- **LSE**: $E_D(w) = \frac{1}{2}\sum_{n=1}^{N} (y_n - w^T\phi(x_n))^2$
- **Ridge (L2)**: $E(w) = E_D(w) + \frac{\lambda}{2} w^T w$
- **Lasso (L1)**: $E(w) = E_D(w) + \frac{\lambda}{2}|w|$

**è§£æè§£**ï¼š
- **LSE**: $w_{LSE} = (\Phi^T\Phi)^{-1}\Phi^T t$
- **Ridge**: $w_{Ridge} = (\lambda I + \Phi^T\Phi)^{-1}\Phi^T t$

**å„ªåŒ–æ–¹æ³•**ï¼š
- **GD**: $w^{(t+1)} = w^{(t)} - \eta \nabla E(w^{(t)})$
- **Newton**: $w^{(t+1)} = w^{(t)} - H^{-1}\nabla E(w^{(t)})$
  - Hessian: $H = \Phi^T\Phi$ (for LSE)

---

#### (2) Probability Distributions

**é›¢æ•£åˆ†ä½ˆ**ï¼š

1. **Bernoulliï¼ˆä¼¯åŠªåˆ©ï¼‰**
   $$Bern(x|\mu) = \mu^x (1-\mu)^{1-x}$$
   - å–®æ¬¡è©¦é©—ï¼ŒæˆåŠŸæ©Ÿç‡ $\mu$

2. **Binomialï¼ˆäºŒé …å¼ï¼‰**
   $$Bin(k|N,\mu) = \binom{N}{k}\mu^k(1-\mu)^{N-k}$$
   - N æ¬¡ç¨ç«‹è©¦é©—ï¼Œk æ¬¡æˆåŠŸ

3. **Poissonï¼ˆæ³Šæ¾ï¼‰**
   $$P(k|\lambda) = e^{-\lambda}\frac{\lambda^k}{k!}$$
   - å–®ä½æ™‚é–“å…§äº‹ä»¶ç™¼ç”Ÿ k æ¬¡çš„æ©Ÿç‡

**é€£çºŒåˆ†ä½ˆ**ï¼š

1. **Betaï¼ˆè²å¡”ï¼‰**
   $$Beta(\mu|a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}$$
   - å®šç¾©åœ¨ [0,1]ï¼Œæ˜¯ Binomial çš„å…±è»›å…ˆé©—

2. **Gammaï¼ˆä¼½ç‘ªï¼‰**
   $$Gam(\lambda|a,b) \propto \lambda^{a-1}e^{-b\lambda}$$
   - å®šç¾©åœ¨ $(0,\infty)$ï¼Œæ˜¯ Poisson çš„å…±è»›å…ˆé©—

3. **Gaussianï¼ˆé«˜æ–¯ï¼‰**
   - 1D: $N(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
   - Multi-D: $N(x|\mu,\Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$

**å…±è»›é—œä¿‚**ï¼š

- **Betaâ€“Binomial**
  - Prior: $Beta(\mu|a,b)$
  - Likelihood: $Bin(m|N,\mu)$
  - Posterior: $Beta(\mu|a+m, b+l)$ where $l=N-m$

- **Gammaâ€“Poisson**
  - Prior: $Gam(\lambda|a,b)$
  - Likelihood: $\prod_{n=1}^{N} Poisson(k_n|\lambda)$
  - Posterior: $Gam(\lambda|a+\sum k_i, b+N)$

**Affine Property of Gaussian**ï¼š
- If $x \sim N(\mu, \Sigma)$ and $y = Ax + b$
- Then $y \sim N(A\mu + b, A\Sigma A^T)$

---

#### (3) MLE, MAP & Bayesian Regression

**Maximum Likelihood Estimation (MLE)**ï¼š
$$\theta_{ML} = \arg\max_\theta P(D|\theta) = \arg\max_\theta \log P(D|\theta)$$

**Gaussian MLE**ï¼š
$$\mu_{ML} = \frac{1}{N}\sum_{n=1}^{N} x_n, \quad \sigma_{ML}^2 = \frac{1}{N}\sum_{n=1}^{N}(x_n - \mu_{ML})^2$$

**Maximum A Posteriori (MAP)**ï¼š
$$\theta_{MAP} = \arg\max_\theta P(\theta|D) = \arg\max_\theta P(D|\theta)P(\theta)$$

**L2 Regularization as MAP**ï¼š
- Likelihood: $P(t|\Phi, w, \beta) = N(t|\Phi w, \beta^{-1}I)$
- Prior: $P(w|\alpha) = N(w|0, \alpha^{-1}I)$
- Result: Ridge regression with $\lambda = \alpha/\beta$

**Bayesian Linear Regression**ï¼š

Prior:
$$P(w) = N(w|m_0, S_0)$$

Posterior:
$$P(w|t) = N(w|m_N, S_N)$$

where:
$$S_N^{-1} = S_0^{-1} + \beta \Phi^T\Phi$$
$$m_N = S_N(S_0^{-1}m_0 + \beta\Phi^Tt)$$

**Predictive Distribution**ï¼š
$$p(t_{new}|t, \phi_{new}) = N(t_{new}|m_N^T\phi_{new}, \sigma_N^2(\phi_{new}))$$
$$\sigma_N^2(\phi_{new}) = \frac{1}{\beta} + \phi_{new}^T S_N \phi_{new}$$

**Sequential Estimation**ï¼š
$$\mu_{ML}^{(N)} = \mu_{ML}^{(N-1)} + \frac{1}{N}(x_N - \mu_{ML}^{(N-1)})$$

é€™å€‹å…¬å¼é¡¯ç¤ºï¼šæ–°çš„ä¼°è¨ˆ = èˆŠçš„ä¼°è¨ˆ + èª¿æ•´é‡

---

### ğŸ§© Side Bï¼šClassification, Clustering & EM

#### (1) Naive Bayes Classifier

**ä¸€èˆ¬å½¢å¼**ï¼š
$$\hat{y} = \arg\max_k P(C_k) \prod_{i=1}^{D} P(x_i|C_k)$$

**é›¢æ•£ç‰¹å¾µ**ï¼ˆLaplace Smoothingï¼‰ï¼š
$$P(x_i=j|C_k) = \frac{\text{Count}(x_i=j \text{ in } C_k) + 1}{\text{Count}(C_k) + \text{TotalBins}}$$

**é€£çºŒç‰¹å¾µ**ï¼ˆGaussian Assumptionï¼‰ï¼š
$$P(x_i|C_k) = N(x_i|\mu_{ik}, \sigma_{ik}^2)$$

**è¨“ç·´æ­¥é©Ÿ**ï¼š
1. è¨ˆç®—æ¯å€‹é¡åˆ¥çš„å…ˆé©— $P(C_k) = \frac{N_k}{N}$
2. å°æ¯å€‹ç‰¹å¾µï¼Œè¨ˆç®—æ¢ä»¶æ©Ÿç‡ $P(x_i|C_k)$
3. é æ¸¬æ™‚ä½¿ç”¨è²è‘‰æ–¯è¦å‰‡

---

#### (2) Logistic Regression

**Sigmoid Function**ï¼š
$$\sigma(a) = \frac{1}{1+e^{-a}}$$

**Properties**ï¼š
- $\sigma(-a) = 1 - \sigma(a)$
- $\frac{d\sigma}{da} = \sigma(a)(1-\sigma(a))$

**Model**ï¼š
$$y_n = P(C_1|\phi_n) = \sigma(w^T\phi_n)$$

**Cross-Entropy Error**ï¼š
$$E(w) = -\sum_{n=1}^{N}[t_n\ln y_n + (1-t_n)\ln(1-y_n)]$$

**Gradient**ï¼š
$$\nabla E(w) = \sum_{n=1}^{N}(y_n - t_n)\phi_n = \Phi^T(y - t)$$

**Hessian**ï¼š
$$H = \nabla\nabla E(w) = \Phi^T R \Phi$$
where $R = \text{diag}(y_n(1-y_n))$

**IRLS (Iteratively Reweighted Least Squares)**ï¼š
$$w^{(new)} = w^{(old)} - H^{-1}\nabla E(w^{(old)})$$
$$w^{(new)} = (\Phi^T R \Phi)^{-1}\Phi^T R z$$
where $z = \Phi w^{(old)} - R^{-1}(y-t)$

---

#### (3) EM Algorithm

**ç›®æ¨™**ï¼šæœ€å¤§åŒ– $P(X|\theta)$ ç•¶æœ‰æ½›åœ¨è®Šæ•¸ $Z$ æ™‚

**E-Step**ï¼š
$$Q(\theta, \theta^{old}) = \sum_Z P(Z|X,\theta^{old}) \log P(X,Z|\theta)$$
$$\gamma(z_k) = P(Z=k|X, \theta^{old})$$

**M-Step**ï¼š
$$\theta^{new} = \arg\max_\theta Q(\theta, \theta^{old})$$

**GMM (Gaussian Mixture Model) ç¯„ä¾‹**ï¼š

**E-step** (è¨ˆç®— responsibilities)ï¼š
$$\gamma(z_{nk}) = \frac{\pi_k N(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^{K}\pi_j N(x_n|\mu_j,\Sigma_j)}$$

**M-step** (æ›´æ–°åƒæ•¸)ï¼š
$$N_k = \sum_{n=1}^{N}\gamma(z_{nk})$$

$$\pi_k^{new} = \frac{N_k}{N}$$

$$\mu_k^{new} = \frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})x_n$$

$$\Sigma_k^{new} = \frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})(x_n-\mu_k^{new})(x_n-\mu_k^{new})^T$$

**æ”¶æ–‚åˆ¤æ–·**ï¼š
ç›£æ§å°æ•¸ä¼¼ç„¶ $\log P(X|\theta)$ æ˜¯å¦ä¸å†é¡¯è‘—å¢åŠ 

---

#### (4) å…¶ä»–é‡è¦æ¦‚å¿µ

**Entropyï¼ˆç†µï¼‰**ï¼š
$$H(X) = -\sum_{i=1}^{n} P(x_i)\log_2 P(x_i)$$
- æ¸¬é‡ä¸ç¢ºå®šæ€§æˆ–ä¿¡æ¯é‡
- ç†µè¶Šé«˜ï¼Œä¸ç¢ºå®šæ€§è¶Šå¤§

**KL Divergenceï¼ˆKL æ•£åº¦ï¼‰**ï¼š
$$KL(P||Q) = \sum_{i=1}^{n} P(x_i)\log\frac{P(x_i)}{Q(x_i)}$$
- æ¸¬é‡å…©å€‹åˆ†ä½ˆçš„å·®ç•°
- éå°ç¨±ï¼š$KL(P||Q) \neq KL(Q||P)$
- éè² ï¼š$KL(P||Q) \geq 0$

**Performance Metrics**ï¼š

From Confusion Matrix:

|           | Predict 1 | Predict 0 |
|-----------|-----------|-----------|
| Actual 1  | TP        | FN        |
| Actual 0  | FP        | TN        |

- **Accuracy**: $\frac{TP + TN}{TP + TN + FP + FN}$
- **Precision**: $\frac{TP}{TP + FP}$
- **Recall/Sensitivity**: $\frac{TP}{TP + FN}$
- **Specificity**: $\frac{TN}{TN + FP}$
- **F1-Score**: $\frac{2 \times Precision \times Recall}{Precision + Recall}$

---

## ğŸ¯ è€ƒè©¦æº–å‚™å»ºè­°

### å¿…é ˆè¦ªæ‰‹æ¨å°çš„å…¬å¼

1. **Linear Regression**
   - LSE çš„è§£æè§£æ¨å°
   - Ridge Regression çš„è§£æè§£æ¨å°
   - å¾ MAP è§’åº¦æ¨å° L2 æ­£è¦åŒ–

2. **Bayesian Methods**
   - Betaâ€“Binomial å…±è»›é—œä¿‚è­‰æ˜
   - Gammaâ€“Poisson å…±è»›é—œä¿‚è­‰æ˜
   - Bayesian Linear Regression çš„ Posterior æ¨å°

3. **Classification**
   - Naive Bayes çš„æ¨å°å’Œæ‡‰ç”¨
   - Logistic Regression çš„æ¢¯åº¦è¨ˆç®—
   - Logistic Regression çš„ Hessian çŸ©é™£æ¨å°

4. **EM Algorithm**
   - GMM çš„ E-step æ¨å°
   - GMM çš„ M-step æ¨å°
   - ç†è§£ç‚ºä»€éº¼ EM æœƒæ”¶æ–‚

5. **Sequential Estimation**
   - å¾ MLE æ¨å° Sequential Update å…¬å¼
   - ç†è§£ç‚ºä»€éº¼å¯ä»¥é€æ­¥æ›´æ–°

### é‡é»æ¦‚å¿µè¤‡ç¿’

1. **ç†è§£æ•¸å­¸èˆ‡ç›´è¦ºçš„å°æ‡‰**
   - æ¯å€‹å…¬å¼èƒŒå¾Œçš„ç‰©ç†/å¹¾ä½•æ„ç¾©
   - åƒæ•¸å°æ¨¡å‹çš„å½±éŸ¿

2. **æŒæ¡æ©Ÿç‡åˆ†ä½ˆä¹‹é–“çš„é—œä¿‚**
   - å…±è»›é—œä¿‚çš„æ„ç¾©å’Œæ‡‰ç”¨
   - å„ç¨®åˆ†ä½ˆé©ç”¨çš„å ´æ™¯

3. **å„ªåŒ–æ–¹æ³•çš„å·®ç•°**
   - GD vs Newton's Method
   - ä½•æ™‚è©²ç”¨å“ªç¨®æ–¹æ³•

4. **Bias-Variance Trade-off**
   - åœ¨ä¸åŒå•é¡Œä¸­å¦‚ä½•é«”ç¾
   - å¦‚ä½•å¹³è¡¡

### å¯¦ä½œç·´ç¿’å»ºè­°

é‹è¡Œæ‰€æœ‰è¦–è¦ºåŒ–è…³æœ¬ï¼Œè§€å¯Ÿï¼š
1. åƒæ•¸è®ŠåŒ–å°çµæœçš„å½±éŸ¿
2. ä¸åŒæ–¹æ³•çš„æ”¶æ–‚é€Ÿåº¦
3. éåº¦æ“¬åˆå’Œæ¬ æ“¬åˆçš„è¦–è¦ºåŒ–å·®ç•°

---

## ğŸ“š è¦–è¦ºåŒ–è…³æœ¬ä½¿ç”¨èªªæ˜

æœ¬ç­†è¨˜é…å‚™å®Œæ•´çš„ Python è¦–è¦ºåŒ–è…³æœ¬ï¼Œå¹«åŠ©ä½ ç›´è§€ç†è§£æ‰€æœ‰æ¦‚å¿µã€‚

### å®‰è£ä¾è³´

```bash
# ä½¿ç”¨ uv å®‰è£æ‰€æœ‰ä¾è³´
uv sync
```

### ç”Ÿæˆæ‰€æœ‰è¦–è¦ºåŒ–

```bash
# é‹è¡Œä¸»è…³æœ¬ç”Ÿæˆæ‰€æœ‰åœ–ç‰‡å’Œå½±ç‰‡
uv run python main.py
```

### å–®ç¨é‹è¡Œç‰¹å®šä¸»é¡Œ

```bash
# å›æ­¸ç›¸é—œè¦–è¦ºåŒ–
uv run python visualizations/regression.py

# æ©Ÿç‡åˆ†ä½ˆè¦–è¦ºåŒ–
uv run python visualizations/distributions.py

# åˆ†é¡æ–¹æ³•è¦–è¦ºåŒ–
uv run python visualizations/classification.py

# EM æ¼”ç®—æ³•è¦–è¦ºåŒ–
uv run python visualizations/em_algorithm.py

# Bias-Variance Trade-off
uv run python visualizations/bias_variance.py
```

### è¼¸å‡ºä½ç½®

æ‰€æœ‰ç”Ÿæˆçš„åœ–ç‰‡å’Œå½±ç‰‡æœƒå„²å­˜åœ¨ `output/` ç›®éŒ„ä¸‹ï¼š
- `output/images/` - æ‰€æœ‰éœæ…‹åœ–ç‰‡
- `output/videos/` - æ‰€æœ‰å‹•ç•«å½±ç‰‡

---

## âœ… å­¸ç¿’æª¢æŸ¥æ¸…å–®

ä½¿ç”¨ä»¥ä¸‹æ¸…å–®æª¢æŸ¥ä½ çš„å­¸ç¿’é€²åº¦ï¼š

### åŸºç¤æ¦‚å¿µ
- [ ] ç†è§£ LSE çš„åˆç†æ€§ï¼ˆé«˜æ–¯å‡è¨­ï¼‰
- [ ] ç†è§£ L1 vs L2 æ­£è¦åŒ–çš„å·®ç•°
- [ ] èƒ½è§£é‡‹ Bias-Variance Trade-off
- [ ] ç†è§£æ¢¯åº¦ä¸‹é™çš„é‹ä½œåŸç†
- [ ] çŸ¥é“ä½•æ™‚ç”¨ Newton's Method

### æ©Ÿç‡èˆ‡çµ±è¨ˆ
- [ ] èƒ½å¯«å‡ºè²è‘‰æ–¯å®šç†ä¸¦è§£é‡‹å„é …
- [ ] ç†è§£ MLE å’Œ MAP çš„å·®ç•°
- [ ] æŒæ¡ Beta-Binomial å…±è»›é—œä¿‚
- [ ] æŒæ¡ Gamma-Poisson å…±è»›é—œä¿‚
- [ ] ç†è§£é«˜æ–¯åˆ†ä½ˆçš„ Affine æ€§è³ª

### åˆ†é¡æ–¹æ³•
- [ ] èƒ½æ¨å° Naive Bayes åˆ†é¡å™¨
- [ ] ç†è§£ç‚ºä»€éº¼ Logistic Regression ç”¨æ–¼åˆ†é¡
- [ ] èƒ½è¨ˆç®— Logistic Regression çš„æ¢¯åº¦
- [ ] ç†è§£æ··æ·†çŸ©é™£çš„å„å€‹æŒ‡æ¨™
- [ ] çŸ¥é“ä½•æ™‚ç”¨ Precisionï¼Œä½•æ™‚ç”¨ Recall

### é€²éšæ–¹æ³•
- [ ] ç†è§£ EM æ¼”ç®—æ³•çš„ E-step å’Œ M-step
- [ ] èƒ½æ¨å° GMM çš„ EM æ›´æ–°å…¬å¼
- [ ] ç†è§£ Sequential Estimation çš„æ„ç¾©
- [ ] èƒ½è¨ˆç®— KL Divergence å’Œ Entropy

### å¯¦ä½œèƒ½åŠ›
- [ ] èƒ½ç”¨ Python å¯¦ç¾ç·šæ€§å›æ­¸
- [ ] èƒ½ç”¨ Python å¯¦ç¾ Naive Bayes
- [ ] èƒ½ç”¨ Python å¯¦ç¾ Logistic Regression
- [ ] èƒ½è¦–è¦ºåŒ– Bias-Variance Trade-off
- [ ] èƒ½å¯¦ç¾ç°¡å–®çš„ EM æ¼”ç®—æ³•

---

## ğŸ”— å»¶ä¼¸å­¸ç¿’è³‡æº

1. **ç¶“å…¸æ•™æ**
   - Pattern Recognition and Machine Learning (Bishop)
   - The Elements of Statistical Learning (Hastie, Tibshirani, Friedman)

2. **ç·šä¸Šèª²ç¨‹**
   - Andrew Ng's Machine Learning Course
   - StatQuest çš„ YouTube é »é“ï¼ˆæ¦‚å¿µè§£é‡‹éå¸¸ç›´è§€ï¼‰

3. **å¯¦ä½œç·´ç¿’**
   - Kaggle ç«¶è³½å’Œæ•™å­¸
   - Scikit-learn å®˜æ–¹æ•™ç¨‹

---

**ç¥ä½ æœŸä¸­è€ƒè©¦é †åˆ©ï¼ ğŸ“šâœ¨**

è¨˜ä½ï¼šæ©Ÿå™¨å­¸ç¿’çš„æ ¸å¿ƒåœ¨æ–¼ç†è§£æ¦‚å¿µï¼Œè€Œéæ­»è¨˜å…¬å¼ã€‚å¤šæ¨å°ã€å¤šè¦–è¦ºåŒ–ã€å¤šæ€è€ƒã€Œç‚ºä»€éº¼ã€ï¼Œä½ ä¸€å®šèƒ½æŒæ¡é€™äº›å…§å®¹ï¼
